{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "def index_documents(documents):\n",
    "    index = defaultdict(list)\n",
    "    for doc_id, doc_text in documents.items():\n",
    "        tokens = word_tokenize(doc_text.lower())  # Tokenize and convert to lowercase\n",
    "        for token in tokens:\n",
    "            index[token].append(doc_id)\n",
    "    \n",
    "    return index\n",
    "# Example documents\n",
    "documents = {\n",
    "    1: \"This is the first document.\",\n",
    "    2: \"Second document for testing purposes.\",\n",
    "    3: \"Another example document.\"\n",
    "}\n",
    "\n",
    "# Indexing the documents\n",
    "index = index_documents(documents)\n",
    "\n",
    "# Example: Retrieve documents containing the token \"document\"\n",
    "print(index[\"document\"])  # Output: [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = {\n",
    "    'this': [1],\n",
    "    'is': [1],\n",
    "    'the': [1],\n",
    "    'first': [1],\n",
    "    'document': [1],\n",
    "    'second': [2],\n",
    "    'for': [2],\n",
    "    'testing': [2],\n",
    "    'purposes': [2],\n",
    "    'another': [3],\n",
    "    'example': [3]\n",
    "}\n",
    "\n",
    "def query_llm(query, index):\n",
    "    tokens = word_tokenize(query.lower())  # Tokenize and convert to lowercase\n",
    "    results = set(index.get(token, []) for token in tokens)  # Get document IDs for each token\n",
    "    # Flatten the list of lists and return unique document IDs\n",
    "    return sorted(set(doc_id for sublist in results for doc_id in sublist))\n",
    "\n",
    "# Example usage:\n",
    "query = \"example document\"\n",
    "query_result = query_llm(query, index)\n",
    "print(\"Query:\", query)\n",
    "print(\"Matching Documents:\", query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    # Placeholder function: you would use an actual tokenizer to count tokens\n",
    "    # For example, use the tokenizer from the transformers library\n",
    "    # from transformers import GPT2Tokenizer\n",
    "    # tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    # return len(tokenizer.encode(text))\n",
    "    return len(text.split())  # Simplified for this example\n",
    "\n",
    "def split_text_into_chunks(text, token_limit):\n",
    "    if count_tokens(text) <= token_limit:\n",
    "        return [text]\n",
    "    \n",
    "    # Find the midpoint to split the text\n",
    "    midpoint = len(text) // 2\n",
    "    \n",
    "    # Split the text at the midpoint\n",
    "    left_part = text[:midpoint]\n",
    "    right_part = text[midpoint:]\n",
    "    \n",
    "    # Ensure we split at a word boundary to avoid cutting words in half\n",
    "    while not left_part.endswith(' ') and midpoint > 0:\n",
    "        midpoint -= 1\n",
    "        left_part = text[:midpoint]\n",
    "        right_part = text[midpoint:]\n",
    "    \n",
    "    # Recursively split the text parts\n",
    "    left_chunks = split_text_into_chunks(left_part.strip(), token_limit)\n",
    "    right_chunks = split_text_into_chunks(right_part.strip(), token_limit)\n",
    "    \n",
    "    return left_chunks + right_chunks\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a very long text that needs to be split into smaller chunks. Each chunk should not exceed the token limit.\"\n",
    "token_limit = 10\n",
    "\n",
    "chunks = split_text_into_chunks(text, token_limit)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
