{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from langchain_community.document_loaders import DirectoryLoader, JSONLoader, TextLoader, UnstructuredFileLoader, UnstructuredHTMLLoader, UnstructuredMarkdownLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_community.embeddings import OllamaEmbeddings\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> Downloading ollama...\n","######################################################################## 100.0%\n",">>> Installing ollama to /usr/local/bin...\n",">>> Adding ollama user to render group...\n",">>> Adding ollama user to video group...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n",">>> Enabling and starting ollama service...\n",">>> NVIDIA GPU installed.\n","Error: listen tcp 127.0.0.1:11434: bind: address already in use\n","\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n","pulling 819c2adf5ce6... 100% ▕████████████████▏ 669 MB                         \n","pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n","pulling b837481ff855... 100% ▕████████████████▏   16 B                         \n","pulling 38badd946f91... 100% ▕████████████████▏  408 B                         \n","verifying sha256 digest \n","writing manifest \n","removing any unused layers \n","success \u001b[?25h\n","\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n","pulling cb1350311b4e... 100% ▕████████████████▏ 9.2 GB                         \n","pulling 7e0503625fed... 100% ▕████████████████▏ 2.9 KB                         \n","pulling 22a7b312010d... 100% ▕████████████████▏  106 B                         \n","pulling 90840d4d4036... 100% ▕████████████████▏ 1.8 KB                         \n","pulling edae7cd8e7c9... 100% ▕████████████████▏  486 B                         \n","verifying sha256 digest \n","writing manifest \n","removing any unused layers \n","success \u001b[?25h\n"]}],"source":["!curl -fsSL https://ollama.com/install.sh | sh\n","!export OLLAMA_HOST=localhost:8888\n","!ollama serve\n","!ollama pull mxbai-embed-large\n","!ollama pull jimscard/whiterabbit-neo"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def get_file_types(directory):\n","        \"\"\"\n","        Traverses all of the files in specified directory and returns types of files that it finds\n","\n","        Args:\n","            directory (str): Path to directory\n","\n","        Returns:\n","            Set[str]: All of the file types that can be found in the directory\n","        \"\"\"\n","        file_types = set()\n","\n","        for filename in os.listdir(directory):\n","                if os.path.isfile(os.path.join(directory, filename)):\n","                        _, ext = os.path.splitext(filename)\n","                        file_types.add(ext)\n","        return file_types"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Specified loader for each type of file found in the cyber data directory (so far)\n","loaders = {\n","    '.php': UnstructuredFileLoader,\n","    '.cs': UnstructuredFileLoader,\n","    '': UnstructuredFileLoader,\n","    '.c': UnstructuredFileLoader,\n","    '.html': UnstructuredHTMLLoader,\n","    '.md': UnstructuredMarkdownLoader,\n","    '.tzt': UnstructuredFileLoader,\n","    '.java': UnstructuredFileLoader,\n","    '.txt': TextLoader,\n","    '.ps1': UnstructuredFileLoader,\n","    '.delphi': UnstructuredFileLoader,\n","    '.asm': UnstructuredFileLoader,\n","    '.TXT': TextLoader,\n","    '.json': JSONLoader\n","}"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def create_directory_loader(file_type, directory_path):\n","        \"\"\"\n","        Creates and returns a DirectoryLoader using the loader specific to the file type provided\n","        \n","        Args:\n","            file_type (str): Type of file to make loader for\n","            directory_path (str): Path to directory\n","\n","        Returns:\n","            DirectoryLoader: loader for the files in the directory provided\n","        \"\"\"\n","        if file_type == '.json':\n","            loader_list = []\n","            for file_name in [file for file in os.listdir(directory_path) if file.endswith('.json')]:\n","                loader_list.append(JSONLoader(file_path=directory_path+'/'+file_name,jq_schema='.', text_content=False))\n","            return loader_list\n","        else:\n","            return DirectoryLoader(\n","            path=directory_path,\n","            glob=f\"**/*{file_type}\",\n","            loader_cls=loaders.get(file_type, UnstructuredFileLoader))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def split_text(docs, chunk_size=512, chunk_overlap=50):\n","        \"\"\"\n","        Splits the given text into chunks of a specified maximum length using RecursiveCharacterTextSplitter.\n","        \n","        Parameters:\n","                text (str): The input text to be split.\n","                max_length (int): The maximum length of each chunk.\n","                chunk_overlap (int): The number of characters to overlap between chunks.\n","                \n","        Returns:\n","                List[str]: A list of text chunks.\n","        \"\"\"\n","        splitter = RecursiveCharacterTextSplitter(\n","                chunk_size=chunk_size,\n","                chunk_overlap=chunk_overlap\n","        )\n","        chunks = splitter.split_documents(docs)\n","        return chunks"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["if __name__==\"__main__\":\n","        DB_FAISS_PATH = '../vectorstore'\n","        DATA_PATH = '../../cyber_data'\n","        file_types = get_file_types(DATA_PATH)\n","        documents = []\n","        \n","        for file_type in file_types:\n","                if file_type.strip() != \"\":\n","                        if file_type == '.json':\n","                                loader_list = create_directory_loader(file_type, DATA_PATH)\n","                                for loader in loader_list:\n","                                        docs = loader.load()\n","                                        chunks = split_text(docs)\n","                                        if chunks != None and chunks != \"\" and len(chunks) > 0:\n","                                                documents.extend(chunks)\n","                        else:        \n","                                loader = create_directory_loader(file_type, DATA_PATH)\n","                                docs = loader.load()\n","                                chunks = split_text(docs)\n","                                if chunks != None and chunks != \"\" and len(chunks) > 0:\n","                                        documents.extend(chunks)\n","                                \n","        # embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\", show_progress=True)\n","        # vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n","        # vectorstore.save_local(DB_FAISS_PATH)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024/07/22 16:31:44 routes.go:1096: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/home/ubuntu/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\n","time=2024-07-22T16:31:44.579Z level=INFO source=images.go:778 msg=\"total blobs: 14\"\n","time=2024-07-22T16:31:44.580Z level=INFO source=images.go:785 msg=\"total unused blobs removed: 0\"\n","time=2024-07-22T16:31:44.580Z level=INFO source=routes.go:1143 msg=\"Listening on 127.0.0.1:11434 (version 0.2.7)\"\n","time=2024-07-22T16:31:44.581Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama1455860553/runners\n","time=2024-07-22T16:31:48.669Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]\"\n","time=2024-07-22T16:31:48.669Z level=INFO source=gpu.go:205 msg=\"looking for compatible GPUs\"\n","time=2024-07-22T16:31:48.871Z level=INFO source=types.go:105 msg=\"inference compute\" id=GPU-8e8e7c36-8c0a-91b2-4bf8-53805472658c library=cuda compute=7.0 driver=12.2 name=\"Tesla V100-SXM2-16GB\" total=\"15.8 GiB\" available=\"15.5 GiB\"\n"]}],"source":["os.system(\"curl -fsSL https://ollama.com/install.sh | sh\")\n","os.system(\"export OLLAMA_HOST=127.0.0.1:33020 ollama list\")\n","os.system(\"sudo service ollama stop\")\n","os.system(\"ollama serve\")\n","os.system(\"ollama pull mxbai-embed-large\")\n","os.system(\"ollama pull jimscard/whiterabbit-neo\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
