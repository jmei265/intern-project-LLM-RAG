{"cells":[{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[],"source":["from langchain_community.document_loaders import DirectoryLoader, JSONLoader, PyPDFLoader, TextLoader, UnstructuredFileLoader, UnstructuredHTMLLoader, UnstructuredMarkdownLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_community.embeddings import OllamaEmbeddings\n","from langchain_community.document_transformers import DoctranPropertyExtractor\n","import os\n","import pathlib\n","import subprocess"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[],"source":["def setup_ollama():\n","        \"\"\"\n","        Downloads (if necessary) and runs ollama locally\n","        \"\"\"\n","        # os.system(\"curl -fsSL https://ollama.com/install.sh | sh\")\n","        # os.system(\"export OLLAMA_HOST=localhost:8888\")\n","        os.system(\"sudo service ollama stop\")\n","        cmd = \"ollama serve\"\n","        with open(os.devnull, 'wb') as devnull:\n","                process = subprocess.Popen(cmd, shell=True, stdout=devnull, stderr=devnull)"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["def txt_file_rename(directory):\n","    \"\"\"\n","    Takes .txt files and renames them if they have a line containing title in them\n","\n","    Args:\n","        directory (str): path to directory where files are stored\n","    \"\"\"\n","    file_paths = pathlib.Path(directory).glob('*.txt')\n","    for file_path in file_paths:\n","        file_name = os.path.basename(file_path)\n","        file_ext = os.path.splitext(file_name)[1]\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            for line in file:\n","                segments = line.split(':')\n","                if 'title' in segments[0].lower() and len(segments) >= 2:\n","                    name = segments[1].strip()\n","                    new_file_name = os.path.join(directory, name + file_ext)\n","                    try:\n","                        os.rename(file_path, new_file_name)\n","                        # print(f'Renamed {file_name} to {name}')\n","                    except FileNotFoundError:\n","                        print(\"\", end='')\n","                        # print(f\"FileNotFoundError: {file_path} not found.\")\n","                    except PermissionError:\n","                        print(\"\", end='')\n","                        # print(\"Permission denied: You don't have the necessary permissions to change the permissions of this file.\")\n","                    except NotADirectoryError:\n","                        print(\"\", end='')\n","                        # print(f\"Not a directory: {new_file_name}\")"]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[],"source":["def get_file_types(directory):\n","        \"\"\"\n","        Traverses all of the files in specified directory and returns types of files that it finds\n","\n","        Args:\n","            directory (str): Path to directory\n","\n","        Returns:\n","            Set[str]: All of the file types that can be found in the directory\n","        \"\"\"\n","        file_types = set()\n","\n","        for filename in os.listdir(directory):\n","                if os.path.isfile(os.path.join(directory, filename)):\n","                        _, ext = os.path.splitext(filename)\n","                        file_types.add(ext)\n","        return file_types"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[],"source":["# Specified loader for each type of file found in the cyber data directory (so far)\n","loaders = {\n","    '.php': UnstructuredFileLoader,\n","    '.cs': UnstructuredFileLoader,\n","    '': UnstructuredFileLoader,\n","    '.c': UnstructuredFileLoader,\n","    '.html': UnstructuredHTMLLoader,\n","    '.md': UnstructuredMarkdownLoader,\n","    '.tzt': UnstructuredFileLoader,\n","    '.java': UnstructuredFileLoader,\n","    '.txt': TextLoader,\n","    '.ps1': UnstructuredFileLoader,\n","    '.delphi': UnstructuredFileLoader,\n","    '.asm': UnstructuredFileLoader,\n","    '.TXT': TextLoader,\n","    '.json': JSONLoader,\n","    '.pdf': PyPDFLoader\n","}"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[],"source":["def create_directory_loader(file_type, directory_path):\n","        \"\"\"\n","        Creates and returns a DirectoryLoader using the loader specific to the file type provided\n","        \n","        Args:\n","            file_type (str): Type of file to make loader for\n","            directory_path (str): Path to directory\n","\n","        Returns:\n","            DirectoryLoader: loader for the files in the directory provided\n","        \"\"\"\n","        if file_type == '.json':\n","            loader_list = []\n","            for file_name in [file for file in os.listdir(directory_path) if file.endswith('.json')]:\n","                loader_list.append(JSONLoader(file_path=directory_path+'/'+file_name,jq_schema='.', text_content=False))\n","            return loader_list\n","        else:\n","            return DirectoryLoader(\n","            path=directory_path,\n","            glob=f\"**/*{file_type}\",\n","            loader_cls=loaders.get(file_type, UnstructuredFileLoader))"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[],"source":["def split_text(docs, chunk_size=512, chunk_overlap=64):\n","        \"\"\"\n","        Splits the given text into chunks of a specified maximum length using RecursiveCharacterTextSplitter.\n","        \n","        Parameters:\n","                text (str): The input text to be split.\n","                max_length (int): The maximum length of each chunk.\n","                chunk_overlap (int): The number of characters to overlap between chunks.\n","                \n","        Returns:\n","                List[str]: A list of text chunks.\n","        \"\"\"\n","        splitter = RecursiveCharacterTextSplitter(\n","                chunk_size=chunk_size,\n","                chunk_overlap=chunk_overlap\n","        )\n","        return splitter.split_documents(docs)"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[],"source":["# def metadata_extractor(documents):\n","#     properties = [\n","#     {\n","#         \"name\": \"category\",\n","#         \"description\": \"What type of document this is.\",\n","#         \"type\": \"string\",\n","#         \"enum\": [\"code_block\", \"instructions\", \"explanation\"],\n","#         \"required\": True,\n","#     },\n","#     {\n","#         \"name\": \"malware\",\n","#         \"description\": \"A list of all malware mentioned in this document.\",\n","#         \"type\": \"array\",\n","#         \"items\": {\n","#             \"name\": \"computer_malware\",\n","#             \"description\": \"The full name of the malware used\",\n","#             \"type\": \"string\",\n","#         },\n","#         \"required\": True,\n","#     },\n","#     {\n","#         \"name\": \"eli5\",\n","#         \"description\": \"Explain this email to me like I'm 5 years old.\",\n","#         \"type\": \"string\",\n","#         \"required\": True,\n","#     },\n","# ]\n","    \n","#     property_extractor = DoctranPropertyExtractor(properties=properties)\n","#     extracted_document = property_extractor.transform_documents(documents, properties=properties)\n","#     return extracted_document"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[],"source":["def chunk_numberer(docs):\n","    \"\"\"\n","    Numbers each chunk in each document to make it easier to find where chunks are located in documents\n","\n","    Args:\n","        docs (List[Document]): list of documents \n","\n","    Returns:\n","        List[Document]: list of documents \n","    \"\"\"\n","    num = 1\n","    if docs:\n","        source = docs[0].metadata.get('source', 'unknown')\n","        for doc in docs:\n","            if 'source' not in doc.metadata:\n","                logger.error(f\"Missing 'source' in document metadata: {doc.metadata}\")\n","                continue\n","            if source != doc.metadata['source']:\n","                num = 1\n","                source = doc.metadata['source']\n","            doc.metadata['chunk_no'] = num\n","            num += 1\n","    return docs"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[],"source":["def document_id(docs):\n","    \"\"\"\n","    Creates unique ID for each chunk based off document name and number of chunk in document\n","\n","    Args:\n","        docs (List[Document]): list of documents\n","\n","    Returns:\n","        List[Document]: list of documents with IDs added\n","    \"\"\"\n","    for doc in docs:\n","        source = os.path.basename(doc.metadata['source'])\n","        chunk_no = doc.metadata['chunk_no']\n","        doc.metadata['id'] = f\"{source}-{chunk_no}\"\n","    return docs"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[],"source":["def load_documents(directory):\n","        \"\"\"\n","        Loads in files from ../data directory and returns them\n","        \n","        Parameters:\n","                directory (str): The input text to be split.\n","        \n","        Returns:\n","                List[Document]: Array of documents\n","        \"\"\"\n","        file_types = get_file_types(directory)\n","        documents = []\n","        \n","        for file_type in file_types:\n","                if file_type.strip() != \"\":\n","                        if file_type == '.json':\n","                                loader_list = create_directory_loader(file_type, directory)\n","                                for loader in loader_list:\n","                                        docs = loader.load()\n","                                        chunks = split_text(docs)\n","                                        if chunks != None and chunks != \"\" and len(chunks) > 0:\n","                                                documents.extend(chunks)\n","                        else:        \n","                                loader = create_directory_loader(file_type, directory)\n","                                docs = loader.load()\n","                                chunks = split_text(docs)\n","                                if chunks != None and chunks != \"\" and len(chunks) > 0:\n","                                        documents.extend(chunks)\n","        \n","        documents = chunk_numberer(documents)\n","        return document_id(documents)"]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[],"source":["def get_file_names(directory):\n","    \"\"\"\n","    Gets names of all the files in the specified directory\n","\n","    Args:\n","        directory (str): path to directory\n","\n","    Returns:\n","        List[str]: list of names of files in directory\n","    \"\"\"\n","    file_names = []\n","    for file_name in os.listdir(directory):\n","        file_names.append(file_name)\n","    return file_names"]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[],"source":["def delete_IDs(directory, vectorstore):\n","    \"\"\"\n","    Deletes all of the documents in pre-existing vectorstore if any of the new documents waiting to be added to the vectorstore share the same name\n","\n","    Args:\n","        directory (str): path to directory\n","        vectorstore (FAISS): FAISS vectorstore\n","\n","    Returns:\n","        FAISS: FAISS vectorstore with duplicate documents deleted\n","    \"\"\"\n","    IDs = []\n","    file_names = get_file_names(directory)\n","    for key, val in vectorstore.docstore._dict.items():\n","        for file_name in file_names:\n","            if file_name in val.metadata['id'].split('-')[0]:\n","                IDs.append(key)\n","    if len(IDs) > 0:\n","        vectorstore.delete(IDs)\n","    return vectorstore"]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[],"source":["def create_knowledgeBase(directory, vector_path):\n","    \"\"\"\n","    Loads in processed documents and used embedding models to embed them, either adding the embeddings to a pre-existing vectorstore or initializes a new vectorstore,\n","    saving the vectorstore locally\n","    \n","    Args:\n","        directory (str): The input text to be split.\n","        vectorstore (FAISS): vector store containing vectors of documents\n","    \"\"\"\n","    documents = load_documents(directory)\n","    os.system(\"ollama pull mxbai-embed-large\")\n","    embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\", show_progress=True)\n","    if len(documents) > 0:\n","        if os.path.exists(vector_path + '/index.faiss'):\n","            vectorstore = FAISS.load_local(vector_path, embeddings, allow_dangerous_deserialization=True)\n","            vectorstore = delete_IDs(directory, vectorstore)\n","            vectorstore.add_documents(documents)\n","            vectorstore.save_local(vector_path)\n","            return vectorstore\n","        else:\n","            vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n","            vectorstore.save_local(vector_path)\n","            return vectorstore"]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[],"source":["def move_files(directory):\n","    \"\"\"\n","    Moves files from unprocessed data directory to processed data directory\n","    \n","    Parameters:\n","        directory (str): The input text to be split.\n","    \"\"\"\n","    file_paths = pathlib.Path(directory).iterdir()\n","    for file_path in file_paths:\n","        new_path = '../../processed_cyber_data/'\n","        file_name = os.path.basename(file_path)\n","        new_path += file_name\n","        os.replace(file_path, new_path)"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n","pulling 819c2adf5ce6... 100% ▕████████████████▏ 669 MB                         \n","pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n","pulling b837481ff855... 100% ▕████████████████▏   16 B                         \n","pulling 38badd946f91... 100% ▕████████████████▏  408 B                         \n","verifying sha256 digest \n","writing manifest \n","removing any unused layers \n","success \u001b[?25h\n","OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  9.42it/s]\n"]}],"source":["if __name__==\"__main__\":\n","        setup_ollama()\n","        DB_FAISS_PATH = '../test_vectorstore'\n","        DATA_PATH = '../test'\n","        txt_file_rename(DATA_PATH)\n","        vectorstore = create_knowledgeBase(DATA_PATH, DB_FAISS_PATH)"]},{"cell_type":"code","execution_count":145,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'langchain_community.vectorstores.faiss.FAISS'>\n"]}],"source":["print(type(vectorstore))"]},{"cell_type":"code","execution_count":144,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["page_content='Hello World!\n","No u' metadata={'source': '../test/test.txt', 'chunk_no': 1, 'id': 'test.txt-1'}\n"]}],"source":["for key, val in vectorstore.docstore._dict.items():\n","    if \"test\" in val.metadata['id'].split('-')[0]:\n","        print(val)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from langchain_community.llms import Ollama\n","# from ragas.testset.generator import TestsetGenerator\n","# from ragas.testset.evolutions import simple, reasoning, multi_context\n","\n","# os.system(\"ollama pull llama3\")\n","# os.system(\"ollama pull jimscard/whiterabbit-neo\")\n","# os.system(\"ollama pull mxbai-embed-large\")\n","# generator_llm = Ollama(model=\"llama3\")\n","# critic_llm = Ollama(model=\"jimscard/whiterabbit-neo\")\n","# embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\", show_progress=True)\n","\n","# generator = TestsetGenerator.from_langchain(\n","#     generator_llm,\n","#     critic_llm,\n","#     embeddings\n","# )\n","\n","# DATA_PATH = '../../processed_cyber_data'\n","# documents = load_documents(DATA_PATH)\n","# testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n","# print(testset)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
