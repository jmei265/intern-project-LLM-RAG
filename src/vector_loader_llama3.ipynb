{"cells":[{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from langchain_community.document_loaders import DirectoryLoader, JSONLoader, TextLoader, UnstructuredFileLoader, UnstructuredHTMLLoader, UnstructuredMarkdownLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_community.embeddings import OllamaEmbeddings\n","import os"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[">>> Downloading ollama...\n","######################################################################## 100.0%\n",">>> Installing ollama to /usr/local/bin...\n",">>> Adding ollama user to render group...\n",">>> Adding ollama user to video group...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n",">>> Enabling and starting ollama service...\n",">>> NVIDIA GPU installed.\n","Error: listen tcp 127.0.0.1:11434: bind: address already in use\n","\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n","pulling 819c2adf5ce6... 100% ▕████████████████▏ 669 MB                         \n","pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n","pulling b837481ff855... 100% ▕████████████████▏   16 B                         \n","pulling 38badd946f91... 100% ▕████████████████▏  408 B                         \n","verifying sha256 digest \n","writing manifest \n","removing any unused layers \n","success \u001b[?25h\n","\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n","pulling cb1350311b4e... 100% ▕████████████████▏ 9.2 GB                         \n","pulling 7e0503625fed... 100% ▕████████████████▏ 2.9 KB                         \n","pulling 22a7b312010d... 100% ▕████████████████▏  106 B                         \n","pulling 90840d4d4036... 100% ▕████████████████▏ 1.8 KB                         \n","pulling edae7cd8e7c9... 100% ▕████████████████▏  486 B                         \n","verifying sha256 digest \n","writing manifest \n","removing any unused layers \n","success \u001b[?25h\n"]}],"source":["!curl -fsSL https://ollama.com/install.sh | sh\n","!export OLLAMA_HOST=localhost:8888\n","!ollama serve\n","!ollama pull mxbai-embed-large\n","!ollama pull jimscard/whiterabbit-neo"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def get_file_types(directory):\n","        \"\"\"\n","        Traverses all of the files in specified directory and returns types of files that it finds\n","\n","        Args:\n","            directory (str): Path to directory\n","\n","        Returns:\n","            Set[str]: All of the file types that can be found in the directory\n","        \"\"\"\n","        file_types = set()\n","\n","        for filename in os.listdir(directory):\n","                if os.path.isfile(os.path.join(directory, filename)):\n","                        _, ext = os.path.splitext(filename)\n","                        file_types.add(ext)\n","        return file_types"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Specified loader for each type of file found in the cyber data directory (so far)\n","loaders = {\n","    '.php': UnstructuredFileLoader,\n","    '.cs': UnstructuredFileLoader,\n","    '': UnstructuredFileLoader,\n","    '.c': UnstructuredFileLoader,\n","    '.html': UnstructuredHTMLLoader,\n","    '.md': UnstructuredMarkdownLoader,\n","    '.tzt': UnstructuredFileLoader,\n","    '.java': UnstructuredFileLoader,\n","    '.txt': TextLoader,\n","    '.ps1': UnstructuredFileLoader,\n","    '.delphi': UnstructuredFileLoader,\n","    '.asm': UnstructuredFileLoader,\n","    '.TXT': TextLoader,\n","    '.json': JSONLoader\n","}"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def create_directory_loader(file_type, directory_path):\n","        \"\"\"\n","        Creates and returns a DirectoryLoader using the loader specific to the file type provided\n","        \n","        Args:\n","            file_type (str): Type of file to make loader for\n","            directory_path (str): Path to directory\n","\n","        Returns:\n","            DirectoryLoader: loader for the files in the directory provided\n","        \"\"\"\n","        return DirectoryLoader(\n","        path=directory_path,\n","        glob=f\"**/*{file_type}\",\n","        loader_cls=loaders.get(file_type, UnstructuredFileLoader)\n",")"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def split_text(docs, chunk_size=512, chunk_overlap=50):\n","        \"\"\"\n","        Splits the given text into chunks of a specified maximum length using RecursiveCharacterTextSplitter.\n","        \n","        Parameters:\n","                text (str): The input text to be split.\n","                max_length (int): The maximum length of each chunk.\n","                chunk_overlap (int): The number of characters to overlap between chunks.\n","                \n","        Returns:\n","                List[str]: A list of text chunks.\n","        \"\"\"\n","        splitter = RecursiveCharacterTextSplitter(\n","                chunk_size=chunk_size,\n","                chunk_overlap=chunk_overlap\n","        )\n","        chunks = splitter.split_documents(docs)\n","        return chunks"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Error loading file ../cyber_data/hugg1_dataset.json\n"]},{"ename":"TypeError","evalue":"JSONLoader.__init__() missing 1 required positional argument: 'jq_schema'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_type\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      9\u001b[0m         loader \u001b[38;5;241m=\u001b[39m create_directory_loader(file_type, DATA_PATH)\n\u001b[0;32m---> 10\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m split_text(docs)\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunks \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m chunks \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunks) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/tensorflow/lib/python3.10/site-packages/langchain_community/document_loaders/directory.py:117\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/tensorflow/lib/python3.10/site-packages/langchain_community/document_loaders/directory.py:195\u001b[0m, in \u001b[0;36mDirectoryLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[0;32m--> 195\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_load_file(i, p, pbar)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[1;32m    198\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n","File \u001b[0;32m/opt/tensorflow/lib/python3.10/site-packages/langchain_community/document_loaders/directory.py:233\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[0;34m(self, item, path, pbar)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 233\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pbar:\n","File \u001b[0;32m/opt/tensorflow/lib/python3.10/site-packages/langchain_community/document_loaders/directory.py:221\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[0;34m(self, item, path, pbar)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m subdoc \u001b[38;5;129;01min\u001b[39;00m loader\u001b[38;5;241m.\u001b[39mlazy_load():\n","\u001b[0;31mTypeError\u001b[0m: JSONLoader.__init__() missing 1 required positional argument: 'jq_schema'"]}],"source":["if __name__==\"__main__\":\n","        DB_FAISS_PATH = '../vectorstore'\n","        DATA_PATH = '../cyber_data'\n","        file_types = get_file_types(DATA_PATH)\n","        documents = []\n","        \n","        for file_type in file_types:\n","                if file_type.strip() != \"\":\n","                        loader = create_directory_loader(file_type, DATA_PATH)\n","                        docs = loader.load()\n","                        chunks = split_text(docs)\n","                        if chunks != None and chunks != \"\" and len(chunks) > 0:\n","                                documents.extend(chunks)\n","                                \n","        embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\", show_progress=True)\n","        vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n","        vectorstore.save_local(DB_FAISS_PATH)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
