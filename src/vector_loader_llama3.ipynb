{"cells":[{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["from langchain_community.document_loaders import DirectoryLoader, JSONLoader, TextLoader, UnstructuredFileLoader, UnstructuredHTMLLoader, UnstructuredMarkdownLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_community.embeddings import OllamaEmbeddings\n","from langchain_community.document_transformers import DoctranPropertyExtractor\n","import os\n","import pathlib\n","import subprocess"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["def setup_ollama():\n","        \"\"\"\n","        Downloads (if necessary) and runs ollama locally\n","        \"\"\"\n","        # os.system(\"curl -fsSL https://ollama.com/install.sh | sh\")\n","        # os.system(\"export OLLAMA_HOST=localhost:8888\")\n","        os.system(\"sudo service ollama stop\")\n","        cmd = \"ollama serve\"\n","        with open(os.devnull, 'wb') as devnull:\n","                process = subprocess.Popen(cmd, shell=True, stdout=devnull, stderr=devnull)"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["def txt_file_rename(directory):\n","    \"\"\"\n","    Takes .txt files and renames them if they have a line containing title in them\n","\n","    Args:\n","        directory (str): path to directory where files are stored\n","    \"\"\"\n","    file_paths = pathlib.Path(directory).glob('*.txt')\n","    for file_path in file_paths:\n","        file_name = os.path.basename(file_path)\n","        file_ext = os.path.splitext(file_name)[1]\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            for line in file:\n","                segments = line.split(':')\n","                if 'title' in segments[0].lower() and len(segments) >= 2:\n","                    name = segments[1].strip()\n","                    new_file_name = os.path.join(directory, name + file_ext)\n","                    try:\n","                        os.rename(file_path, new_file_name)\n","                        # print(f'Renamed {file_name} to {name}')\n","                    except FileNotFoundError:\n","                        print(\"\", end='')\n","                        # print(f\"FileNotFoundError: {file_path} not found.\")\n","                    except PermissionError:\n","                        print(\"\", end='')\n","                        # print(\"Permission denied: You don't have the necessary permissions to change the permissions of this file.\")\n","                    except NotADirectoryError:\n","                        print(\"\", end='')\n","                        # print(f\"Not a directory: {new_file_name}\")"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["def get_file_types(directory):\n","        \"\"\"\n","        Traverses all of the files in specified directory and returns types of files that it finds\n","\n","        Args:\n","            directory (str): Path to directory\n","\n","        Returns:\n","            Set[str]: All of the file types that can be found in the directory\n","        \"\"\"\n","        file_types = set()\n","\n","        for filename in os.listdir(directory):\n","                if os.path.isfile(os.path.join(directory, filename)):\n","                        _, ext = os.path.splitext(filename)\n","                        file_types.add(ext)\n","        return file_types"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["# Specified loader for each type of file found in the cyber data directory (so far)\n","loaders = {\n","    '.php': UnstructuredFileLoader,\n","    '.cs': UnstructuredFileLoader,\n","    '': UnstructuredFileLoader,\n","    '.c': UnstructuredFileLoader,\n","    '.html': UnstructuredHTMLLoader,\n","    '.md': UnstructuredMarkdownLoader,\n","    '.tzt': UnstructuredFileLoader,\n","    '.java': UnstructuredFileLoader,\n","    '.txt': TextLoader,\n","    '.ps1': UnstructuredFileLoader,\n","    '.delphi': UnstructuredFileLoader,\n","    '.asm': UnstructuredFileLoader,\n","    '.TXT': TextLoader,\n","    '.json': JSONLoader\n","}"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["def create_directory_loader(file_type, directory_path):\n","        \"\"\"\n","        Creates and returns a DirectoryLoader using the loader specific to the file type provided\n","        \n","        Args:\n","            file_type (str): Type of file to make loader for\n","            directory_path (str): Path to directory\n","\n","        Returns:\n","            DirectoryLoader: loader for the files in the directory provided\n","        \"\"\"\n","        if file_type == '.json':\n","            loader_list = []\n","            for file_name in [file for file in os.listdir(directory_path) if file.endswith('.json')]:\n","                loader_list.append(JSONLoader(file_path=directory_path+'/'+file_name,jq_schema='.', text_content=False))\n","            return loader_list\n","        else:\n","            return DirectoryLoader(\n","            path=directory_path,\n","            glob=f\"**/*{file_type}\",\n","            loader_cls=loaders.get(file_type, UnstructuredFileLoader))"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["def split_text(docs, chunk_size=512, chunk_overlap=64):\n","        \"\"\"\n","        Splits the given text into chunks of a specified maximum length using RecursiveCharacterTextSplitter.\n","        \n","        Parameters:\n","                text (str): The input text to be split.\n","                max_length (int): The maximum length of each chunk.\n","                chunk_overlap (int): The number of characters to overlap between chunks.\n","                \n","        Returns:\n","                List[str]: A list of text chunks.\n","        \"\"\"\n","        splitter = RecursiveCharacterTextSplitter(\n","                chunk_size=chunk_size,\n","                chunk_overlap=chunk_overlap\n","        )\n","        return splitter.split_documents(docs)"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["# def metadata_extractor(documents):\n","#     properties = [\n","#     {\n","#         \"name\": \"category\",\n","#         \"description\": \"What type of document this is.\",\n","#         \"type\": \"string\",\n","#         \"enum\": [\"code_block\", \"instructions\", \"explanation\"],\n","#         \"required\": True,\n","#     },\n","#     {\n","#         \"name\": \"malware\",\n","#         \"description\": \"A list of all malware mentioned in this document.\",\n","#         \"type\": \"array\",\n","#         \"items\": {\n","#             \"name\": \"computer_malware\",\n","#             \"description\": \"The full name of the malware used\",\n","#             \"type\": \"string\",\n","#         },\n","#         \"required\": True,\n","#     },\n","#     {\n","#         \"name\": \"eli5\",\n","#         \"description\": \"Explain this email to me like I'm 5 years old.\",\n","#         \"type\": \"string\",\n","#         \"required\": True,\n","#     },\n","# ]\n","    \n","#     property_extractor = DoctranPropertyExtractor(properties=properties)\n","#     extracted_document = property_extractor.transform_documents(documents, properties=properties)\n","#     return extracted_document"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["def chunk_numberer(docs):\n","    num = 1\n","    source = docs[0].metadata['source']\n","    for doc in docs:\n","        if source != doc.metadata['source']:\n","            num = 1\n","            source = doc.metadata['source']\n","        doc.metadata['chunk_no'] = num\n","        num += 1\n","    return docs"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["def load_documents(directory):\n","        \"\"\"\n","        Loads in files from ../data directory and returns them\n","        \n","        Parameters:\n","                directory (str): The input text to be split.\n","        \n","        Returns:\n","                List[Document]: Array of documents\n","        \"\"\"\n","        file_types = get_file_types(directory)\n","        documents = []\n","        \n","        for file_type in file_types:\n","                if file_type.strip() != \"\":\n","                        if file_type == '.json':\n","                                loader_list = create_directory_loader(file_type, directory)\n","                                for loader in loader_list:\n","                                        docs = loader.load()\n","                                        chunks = split_text(docs)\n","                                        if chunks != None and chunks != \"\" and len(chunks) > 0:\n","                                                documents.extend(chunks)\n","                        else:        \n","                                loader = create_directory_loader(file_type, directory)\n","                                docs = loader.load()\n","                                chunks = split_text(docs)\n","                                if chunks != None and chunks != \"\" and len(chunks) > 0:\n","                                        documents.extend(chunks)\n","        \n","        documents = chunk_numberer(documents)\n","        return documents\n","        # return metadata_extractor(documents)"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["def create_knowledgeBase(directory, vectorstore):\n","    \"\"\"\n","    Loads in documents, splits into chunks, and vectorizes chunks and stores vectors under FAISS vector store\n","    \n","    Parameters:\n","        directory (str): The input text to be split.\n","        vectorstore (FAISS):\n","    \"\"\"\n","    documents = load_documents(directory)\n","    os.system(\"ollama pull mxbai-embed-large\")\n","    embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\", show_progress=True)\n","    vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n","    if os.path.exists(DB_FAISS_PATH + '/index.faiss'):\n","        old_vectorstore = FAISS.load_local(DB_FAISS_PATH, embeddings)\n","        old_vectorstore.merge_from(vectorstore)\n","        old_vectorstore.save_local(DB_FAISS_PATH)\n","    else:\n","        vectorstore.save_local(DB_FAISS_PATH)"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["def move_files(directory):\n","    \"\"\"\n","    Moves files from unprocessed data directory to processed data directory\n","    \n","    Parameters:\n","        directory (str): The input text to be split.\n","    \"\"\"\n","    file_paths = pathlib.Path(directory).iterdir()\n","    for file_path in file_paths:\n","        new_path = '../../processed_cyber_data/'\n","        file_name = os.path.basename(file_path)\n","        new_path += file_name\n","        os.replace(file_path, new_path)"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n","pulling 819c2adf5ce6... 100% ▕████████████████▏ 669 MB                         \n","pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n","pulling b837481ff855... 100% ▕████████████████▏   16 B                         \n","pulling 38badd946f91... 100% ▕████████████████▏  408 B                         \n","verifying sha256 digest \n","writing manifest \n","removing any unused layers \n","success \u001b[?25h\n","OllamaEmbeddings: 100%|██████████| 20204/20204 [33:28<00:00, 10.06it/s]\n"]}],"source":["if __name__==\"__main__\":\n","        setup_ollama()\n","        DB_FAISS_PATH = '../vectorstore'\n","        DATA_PATH = '../../unprocessed_cyber_data'\n","        txt_file_rename(DATA_PATH)\n","        create_knowledgeBase(DATA_PATH, DB_FAISS_PATH)\n","        move_files(DATA_PATH)"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["# from langchain_community.llms import Ollama\n","# from ragas.testset.generator import TestsetGenerator\n","# from ragas.testset.evolutions import simple, reasoning, multi_context\n","\n","# os.system(\"ollama pull llama3\")\n","# os.system(\"ollama pull jimscard/whiterabbit-neo\")\n","# os.system(\"ollama pull mxbai-embed-large\")\n","# generator_llm = Ollama(model=\"llama3\")\n","# critic_llm = Ollama(model=\"jimscard/whiterabbit-neo\")\n","# embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\", show_progress=True)\n","\n","# generator = TestsetGenerator.from_langchain(\n","#     generator_llm,\n","#     critic_llm,\n","#     embeddings\n","# )\n","\n","# DATA_PATH = '../../processed_cyber_data'\n","# documents = load_documents(DATA_PATH)\n","# testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n","# print(testset)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
