{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Packages used in RAG system"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import streamlit as sl\n", "from langchain_community.document_loaders import DirectoryLoader, TextLoader, UnstructuredFileLoader, UnstructuredHTMLLoader, UnstructuredMarkdownLoader\n", "from langchain.text_splitter import RecursiveCharacterTextSplitter\n", "from langchain_community.vectorstores import FAISS\n", "from langchain.prompts import ChatPromptTemplate\n", "from langchain_core.output_parsers import StrOutputParser\n", "from langchain_core.runnables import RunnablePassthrough\n", "from langchain_community.embeddings import OllamaEmbeddings\n", "from langchain_community.llms import Ollama\n", "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n", "from sentence_transformers import CrossEncoder\n", "from langchain.retrievers.document_compressors import LLMChainExtractor\n", "from langchain_community.document_transformers import DoctranPropertyExtractor\n", "import logging\n", "import os\n", "import pathlib\n", "import subprocess"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Specified loader for each type of file found in the cyber data directory (so far)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loaders = {\n", "    '.php': UnstructuredFileLoader,\n", "    '.cs': UnstructuredFileLoader,\n", "    '': UnstructuredFileLoader,\n", "    '.c': UnstructuredFileLoader,\n", "    '.html': UnstructuredHTMLLoader,\n", "    '.md': UnstructuredMarkdownLoader,\n", "    '.tzt': UnstructuredFileLoader,\n", "    '.java': UnstructuredFileLoader,\n", "    '.txt': TextLoader,\n", "    '.ps1': UnstructuredFileLoader,\n", "    '.delphi': UnstructuredFileLoader,\n", "    '.asm': UnstructuredFileLoader,\n", "    '.TXT': TextLoader\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["logging.basicConfig(level=logging.INFO, filename = 'vector_log.log', filemode = 'w', format='%(name)s - %(levelname)s - %(message)s')\n", "logger = logging.getLogger(__name__)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def setup_ollama():\n", "        \"\"\"\n", "        Downloads (if necessary) and runs ollama locally\n", "        \"\"\"\n", "        # os.system(\"curl -fsSL https://ollama.com/install.sh | sh\")\n", "        # os.system(\"export OLLAMA_HOST=localhost:8888\")\n", "        os.system(\"sudo service ollama stop\")\n", "        cmd = \"ollama serve\"\n", "        with open(os.devnull, 'wb') as devnull:\n", "                process = subprocess.Popen(cmd, shell=True, stdout=devnull, stderr=devnull)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def txt_file_rename(directory):\n", "    \"\"\"\n", "    Takes .txt files and renames them if they have a line containing title in them\n", "    Args:\n", "        directory (str): path to directory where files are stored\n", "    \"\"\"\n", "    file_paths = pathlib.Path(directory).glob('*.txt')\n", "    for file_path in file_paths:\n", "        file_name = os.path.basename(file_path)\n", "        file_ext = os.path.splitext(file_name)[1]\n", "        with open(file_path, 'r', encoding='utf-8') as file:\n", "            for line in file:\n", "                segments = line.split(':')\n", "                if 'title' in segments[0].lower() and len(segments) >= 2:\n", "                    name = segments[1].strip()\n", "                    new_file_name = os.path.join(directory, name + file_ext)\n", "                    try:\n", "                        os.rename(file_path, new_file_name)\n", "                        print(f'Renamed {file_name} to {name}')\n", "                    except FileNotFoundError:\n", "                        print(f\"FileNotFoundError: {file_path} not found.\")\n", "                    except PermissionError:\n", "                        print(\"Permission denied: You don't have the necessary permissions to change the permissions of this file.\")\n", "                    except NotADirectoryError:\n", "                        print(f\"Not a directory: {new_file_name}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_file_types(directory):\n", "        \"\"\"\n", "        Traverses all of the files in specified directory and returns types of files that it finds\n", "        Args:\n", "            directory (str): Path to directory\n", "        Returns:\n", "            Set[str]: All of the file types that can be found in the directory\n", "        \"\"\"\n", "        file_types = set()\n", "        for filename in os.listdir(directory):\n", "                if os.path.isfile(os.path.join(directory, filename)):\n", "                        _, ext = os.path.splitext(filename)\n", "                        file_types.add(ext)\n", "        return file_types"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_directory_loader(file_type, directory_path):\n", "        \"\"\"\n", "        Creates and returns a DirectoryLoader using the loader specific to the file type provided\n", "        \n", "        Args:\n", "            file_type (str): Type of file to make loader for\n", "            directory_path (str): Path to directory\n", "        Returns:\n", "            DirectoryLoader: loader for the files in the directory provided\n", "        \"\"\"\n", "        return DirectoryLoader(\n", "        path=directory_path,\n", "        glob=f\"**/*{file_type}\",\n", "        loader_cls=loaders.get(file_type, UnstructuredFileLoader)\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def split_text(docs, chunk_size=512, chunk_overlap=64):\n", "        \"\"\"\n", "        Splits the given text into chunks of a specified maximum length using RecursiveCharacterTextSplitter.\n", "        \n", "        Parameters:\n", "                text (str): The input text to be split.\n", "                max_length (int): The maximum length of each chunk.\n", "                chunk_overlap (int): The number of characters to overlap between chunks.\n", "                \n", "        Returns:\n", "                List[str]: A list of text chunks.\n", "        \"\"\"\n", "        splitter = RecursiveCharacterTextSplitter(\n", "                chunk_size=chunk_size,\n", "                chunk_overlap=chunk_overlap\n", "        )\n", "        chunks = splitter.split_documents(docs)\n", "        return chunks\n", "    \n", "# def metadata_extractor(documents):\n", "#     properties = [\n", "#     {\n", "#         \"name\": \"category\",\n", "#         \"description\": \"What type of document this is.\",\n", "#         \"type\": \"string\",\n", "#         \"enum\": [\"code_block\", \"instructions\", \"explanation\"],\n", "#         \"required\": True,\n", "#     },\n", "#     {\n", "#         \"name\": \"malware\",\n", "#         \"description\": \"A list of all malware mentioned in this document.\",\n", "#         \"type\": \"array\",\n", "#         \"items\": {\n", "#             \"name\": \"computer_malware\",\n", "#             \"description\": \"The full name of the malware used\",\n", "#             \"type\": \"string\",\n", "#         },\n", "#         \"required\": True,\n", "#     },\n", "#     {\n", "#         \"name\": \"eli5\",\n", "#         \"description\": \"Explain this email to me like I'm 5 years old.\",\n", "#         \"type\": \"string\",\n", "#         \"required\": True,\n", "#     },\n", "# ]\n", "    \n", "#     property_extractor = DoctranPropertyExtractor(properties=properties)\n", "#     extracted_document = property_extractor.transform_documents(documents, properties=properties)\n", "#     return extracted_document"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_documents(directory):\n", "        \"\"\"\n", "        Loads in files from ../data directory and returns them\n", "        Returns:\n", "                List[Document]: Array of documents\n", "        \"\"\"\n", "        file_types = get_file_types(directory)\n", "        documents = []\n", "        \n", "        for file_type in file_types:\n", "                if file_type.strip() != \"\":\n", "                        if file_type == '.json':\n", "                                loader_list = create_directory_loader(file_type, directory)\n", "                                for loader in loader_list:\n", "                                        docs = loader.load()\n", "                                        chunks = split_text(docs)\n", "                                        if chunks != None and chunks != \"\" and len(chunks) > 0:\n", "                                                documents.extend(chunks)\n", "                        else:        \n", "                                loader = create_directory_loader(file_type, directory)\n", "                                docs = loader.load()\n", "                                chunks = split_text(docs)\n", "                                if chunks != None and chunks != \"\" and len(chunks) > 0:\n", "                                        documents.extend(chunks)\n", "        return documents"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_knowledgeBase(directory, vectorstore):\n", "    \"\"\"\n", "    Loads in documents, splits into chunks, and vectorizes chunks and stores vectors under FAISS vector store\n", "    \n", "    Parameters:\n", "        directory (str): The input text to be split.\n", "        vectorstore (FAISS):\n", "    \"\"\"\n", "    documents = load_documents(directory)\n", "    os.system(\"ollama pull mxbai-embed-large\")\n", "    embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\", show_progress=True)\n", "    if len(documents) > 0:\n", "        vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n", "        if os.path.exists(DB_FAISS_PATH + '/index.faiss'):\n", "            old_vectorstore = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n", "            old_vectorstore.merge_from(DB_FAISS_PATH)\n", "            old_vectorstore.save_local(DB_FAISS_PATH)\n", "        else:\n", "            vectorstore.save_local(DB_FAISS_PATH)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def move_files(directory):\n", "    \"\"\"\n", "    Moves files from unprocessed data directory to processed data directory\n", "    \n", "    Parameters:\n", "        directory (str): The input text to be split.\n", "    \"\"\"\n", "    file_paths = pathlib.Path(directory).iterdir()\n", "    for file_path in file_paths:\n", "        new_path = '../../processed_cyber_data/'\n", "        file_name = os.path.basename(file_path)\n", "        new_path += file_name\n", "        os.replace(file_path, new_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_knowledgeBase():\n", "        \"\"\"\n", "        Loads and returns vector store\n", "        Returns:\n", "            FAISS: vector store\n", "        \"\"\"\n", "        os.system(\"ollama pull mxbai-embed-large\")\n", "        embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\", show_progress=True)\n", "        db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n", "        return db\n", "        \n", "def load_llm():\n", "        \"\"\"\n", "        Creates and returns WhiteRabbitNeo model\n", "        Returns:\n", "            WhiteRabbitNeo: LLM\n", "        \"\"\"\n", "        os.system(\"ollama pull jimscard/whiterabbit-neo\")\n", "        llm = Ollama(model=\"jimscard/whiterabbit-neo\")\n", "        return llm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_prompt():\n", "        \"\"\"\n", "        Creates and returns prompt for LLM query that specifies how response sounds and structure of response\n", "        Returns:\n", "            ChatPromptTemplate: Prompt for LLM\n", "        # \"\"\"\n", "        # prompt = \"\"\"You need to answer the question in the sentence as same as in the content.\n", "        # Cite the sources of any data provided.\n", "        # Given below is the context and question of the user.\n", "        # context = {context}\n", "        # question = {question}\n", "        # if the answer is not in the data provided answer \"Sorry, I'm not sure how to respond to this.\"\n", "        # \"\"\"\n", "        \n", "        prompt = \"\"\"Provide detailed steps to address the question about offensive cyber operations.\n", "        Cite the sources of any data provided.\n", "        Given below is the context and question of the user.\n", "        context = {context}\n", "        question = {question}\n", "        if the answer is not in the data provided answer \"Sorry, I'm not sure how to respond to this.\"\n", "        \"\"\"\n", "    \n", "        # prompt = \"\"\"Given the following question and context, extract any part of the \n", "        # context that is relevant to answer the question. \n", "        # Cite the sources of any data provided.\n", "        # Given below is the context and question of the user.\n", "        # context = {context}\n", "        # question = {question}\n", "        # if the answer is not in the data provided answer \"Sorry, I'm not sure how to respond to this.\"\n", "        # \"\"\"\n", "        prompt = ChatPromptTemplate.from_template(prompt)\n", "        return prompt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_reranker():\n", "        \"\"\"\n", "        Creates and returns MixedBread reranker algorithm\n", "        Returns:\n", "            MixedBread: reranker\n", "        \"\"\"\n", "        reranker = CrossEncoder(\"mixedbread-ai/mxbai-rerank-large-v1\")\n", "        return reranker    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def format_docs(docs):\n", "        \"\"\"\n", "        Logs and joins documents retrieved from vector store in one line format to make it easier for LLM to parse\n", "        \n", "        Args:\n", "            docs (Document): Documents from vector stores\n", "        Returns:\n", "            String: documents in one line\n", "        \"\"\"        \n", "        reranker = load_reranker()\n", "        \n", "        docs_content = []\n", "        for doc in docs:\n", "                logger.info(f\"\\nDocument used in query for {query}: {doc}\")\n", "                docs_content.append(str(doc.page_content))\n", "                \n", "        ranked_docs = reranker.rank(query, docs_content, return_documents=True)\n", "        ranked_docs_content = []\n", "        for ranked_doc in ranked_docs:\n", "                ranked_docs_content.append(str(ranked_doc.get('text')))\n", "        \n", "        return \"\\n\\n\".join(ranked_docs_content)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_compressor():\n", "        \"\"\"\n", "        Creates and returns contextual compressor using LLM which reduces size of documents from vector store\n", "        Returns:\n", "            LLMChainExtractor: contextual compressor\n", "        \"\"\"\n", "        llm = load_llm()\n", "        compressor = LLMChainExtractor.from_llm(llm)\n", "        return compressor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def respond_with_sources(query, retriever) -> str:\n", "    \"\"\"\n", "    Pulls and returns the sources of all the documents used in the query\n", "    Args:\n", "        query (str): query inputted by user\n", "        retriever (FAISS_retriver): gets most similar vectors from vector store\n", "    Returns:\n", "        str: each source used\n", "    \"\"\"\n", "    retrieved_docs = retriever.invoke(query)\n", "    sources = {doc.metadata['source'].replace('/', '.').split('.')[-2] for doc in retrieved_docs}\n", "    citation_text = \"Documents used: \" + \", \".join(sources)\n", "    return f\"\\n\\n{citation_text}\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__=='__main__':\n", "        #Pulls and serves ollama for models used later on\n", "        setup_ollama()\n", "        \n", "        # Location of the documents for the vector store and location of the vector store\n", "        DATA_PATH = '../../unprocessed_cyber_data'\n", "        DB_FAISS_PATH = '../vectorstore'\n", "        \n", "        # Creates header for streamlit app and writes to it\n", "        sl.header(\"Welcome to the \ud83d\udcdd Offensive Cyber Assistant\")\n", "        sl.write(\"\ud83e\udd16 You can chat by entering your queries\")\n", "        \n", "        try:\n", "                #Creates vector store using any unprocessed files\n", "                txt_file_rename(DATA_PATH)\n", "                create_knowledgeBase(DATA_PATH, DB_FAISS_PATH)\n", "                move_files(DATA_PATH)\n", "                \n", "                # Loads in vector store, LLM, and prompt\n", "                knowledge_base = load_knowledgeBase()\n", "                llm = load_llm()\n", "                prompt = load_prompt()\n", "                logger.info(\"Components loaded successfully.\")\n", "                \n", "                # Creates text box for user to query data\n", "                query=sl.text_input('Enter some text')\n", "                \n", "                if(query):\n", "                        # Gets most similar vectors from knowledge base to user query and turns into actual documents\n", "                        similar_embeddings=knowledge_base.similarity_search(query)\n", "                        similar_embeddings=FAISS.from_documents(documents=similar_embeddings, embedding=OllamaEmbeddings(model=\"mxbai-embed-large\", show_progress=True))\n", "                        \n", "                        # Defines retriever for getting vectors from vector store\n", "                        retriever = similar_embeddings.as_retriever()\n", "                        compressor = load_compressor()\n", "                        compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n", "                        \n", "                        # Chain that combines query, vectors, prompt, and LLM to generate response\n", "                        rag_chain = (\n", "                                {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n", "                                | prompt\n", "                                | llm\n", "                                | StrOutputParser()\n", "                            )\n", "                        \n", "                        # Calls chain and writes response to streamlit\n", "                        response=rag_chain.invoke(query) + respond_with_sources(query, retriever)\n", "                        sl.write(response)\n", "        \n", "        except Exception as e:\n", "            logger.error(f\"\\nError loading components: {e}\")\n", "            sl.write(\"An error occurred while loading the components. Please check the logs.\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}